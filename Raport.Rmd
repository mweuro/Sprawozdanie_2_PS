---
title: "Sprawozdanie 2"
author: "Szymon Malec, Michał Wiktorowski"
output:
  pdf_document: 
    number_sections: true
    extra_dependencies: ["polski", "mathtools", "amsthm", "amssymb", "icomma", "upgreek", "xfrac", "scrextend", "float", "tabularx", "hyperref", "caption", "enumitem", "titlesec"]
fontsize: 12pt
---

\renewcommand{\figurename}{Wykres}
\renewcommand{\tablename}{Tablica}
\raggedbottom
\titlelabel{\thetitle.\quad}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE, fig.pos = "H")
```

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(knitr)
```




\section{Wstęp}

|       Niniejsza praca poświęcona jest przedstawieniu trzech testów dla parametru proporcji (prawdopodobieństwa sukcesu w próbie Bernoulliego) rozkładu dwumianowego. Dla pewnej realizacji zmiennej losowej $S \sim \mathcal{B}(n, p)$ rozważmy hipotezy:
\begin{itemize}
\item $H_0$: $p = p_0 = 0.5$
\item $H_1$: $p \neq p_0 = 0.5$,
\end{itemize}
gdzie $H_0$ i $H_1$ są odpowiednio hipotezą zerową i alternatywną. Sprawdzimy ich poprawność przy użyciu trzech testów:
\begin{itemize}
\item testu opartego o $\textbf{przedział Wilsona}$,
\item testu opartego o $\textbf{przedział Cloppera-Pearsona}$,
\item testu opartego o $\textbf{przedział Jeffreysa}$,
\end{itemize}
na poziomie istotności $\alpha = 0.05$. Następnie poddamy każdy z testów analizie - zobaczymy, jak zachowuje się ich moc i na tej podstawie stwierdzimy, który test jest najmocniejszy.





\section{Test oparty o przedział Wilsona}

|       Do konstrukcji przedziału wykorzystamy fakt, że rozkład dwumianowy można przybliżać rozkładem normalnym. Przyjmuje się, że przybliżenie to jest dobre, gdy $np > 5$, $n(1 - p) > 5$ oraz wartość $p$ jest bliskia 0.5. Wartość oczekiwana $S$ to $\mathrm{E}S = np$, a odchylenie standardowe równe jest $\mathrm{Std}(S) = \sqrt{np(1-p)}$, zatem dla pewnej zmiennej losowej $Z \sim \mathcal{N}(0, 1)$ możemy powiedzieć, że
$$
\frac{S - np_0}{\sqrt{np_0(1 - p_0)}} \stackrel{\mathrm{d}}{\approx} Z
$$
pod warunkiem, że $H_0$ jest prawdziwa. Korzystając z tego, zapisujemy
$$
\mathrm{P}\left( -z < \frac{S - np_0}{\sqrt{np_0(1 - p_0)}} < z \right) \approx 1 - \alpha,
$$
gdzie $z$ jest kwantylem rzędu $1 - \frac{\alpha}{2}$ rozkładu $\mathcal{N}(0, 1)$. Podnosząc strony nierówności wewnątrz funkcji prawdopodobieństwa do kwadratu dostajemy
$$
\mathrm{P}\left( \frac{(S - np_0)^2}{np_0(1 - p_0)} < z^2 \right) \approx 1 - \alpha,
$$
a następnie rozwiązując ukrytą wewnątrz nierówność kwadratową względem $p_0$ otrzymamy
$$
\mathrm{P}(p_0 \in W) \approx 1 - \alpha,
$$
gdzie
$$
W = \left[ \frac{S + \frac{1}{2}z^2}{n + z^2} - \frac{z}{n + z^2} \sqrt{\frac{S(n - S)}{n} + \frac{z^2}{4}} \ , \ \ \frac{S + \frac{1}{2}z^2}{n + z^2} + \frac{z}{n + z^2} \sqrt{\frac{S(n - S)}{n} + \frac{z^2}{4}} \right].
$$
nazywamy przedziałem Wilsona. Widzimy zatem, że przedział ten będzie zmieniać się w zależności od wartości $S$ i jeśli hipoteza zerowa jest prawdziwa, $p_0$ będzie wpadać do niego z częstością zbliżoną do $1 - \alpha$.






\section{Test oparty o przedział Cloppera-Pearsona}

|       Oznaczmy kwantyl rzędu $\gamma$ rozkładu $\mathcal{B}eta(\alpha, \beta)$ jako $b_{\gamma}(\alpha, \beta)$. Dla pewnej realizacji zmiennej $S$, przedział Cloppera-Pearsona przedstawia się w postaci
$$
CP = \left[ b_{\frac{\alpha}{2}}(S, \ n - S + 1), \ \ b_{1 - \frac{\alpha}{2}}(S + 1, \ n - S) \right],
$$
gdzie
$$
\mathrm{P}(p_0 \in CP) \approx 1 - \alpha
$$
pod warunkiem prawdziwości hipotezy zerowej.



\section{Test oparty o przedział Jeffreysa}
|       Przedział ten definiuje się podobnie jak przedział Cloppera-Pearsona, z tą różnicą że tutaj korzystamy wyłącznie z kwantyli rozkładu $\mathcal{B}(S + 0.5, \ n - S + 0.5)$. Dla pewnej realizacji zmiennej $S$ ma on zatem następującą postać:
$$
J = \left[ b_{\frac{\alpha}{2}}(S + 0.5, \ n - S + 0.5), \ \ b_{1 - \frac{\alpha}{2}}(S + 0.5, \ n - S + 0.5) \right],
$$
gdzie
$$
\mathrm{P}(p_0 \in J) \approx 1 - \alpha
$$
pod warunkiem, że hipoteza $H_0$ jest prawdziwa.



\section{Porównanie testów}
|     W powyższych sekcjach przedstawiliśmy trzy różne sposoby na testowanie parametru proporcji $p$ w rozkładzie dwumianowym. Istnieje jednak pytanie - który z tych testów daje nam największą pewność poprawności hipotezy? Odpowiemy na to pytanie analizując wykresy mocy dla każdego z tych testów. Rozważymy trzy różne długości prób Bernoulliego - $n \in \{7, 35, 250\}$. Dla różnych wartości $0 < p < 1$, będziemy testować hipotezę $H_0 : ~ p = 0,5$. Umożliwi nam to przedstawienie wykresu zależności mocy testu od wartości $p$. W tym celu wykonamy $N = 10000$ symulacji Monte Carlo, aby wyznaczyć możliwie najdokładniejsze wartości mocy rozważanych testów.
```{r}
library(dplyr)
library(ggplot2)
library(zeallot)
library(tidyr)
library(reshape2)
library(cowplot)
library(DescTools)
options(repr.plot.width = 12, repr.plot.height = 8)
```

```{r}
plots <- function(n, p0 = 0.5,
                  alpha = 0.05,
                  z = qnorm(1 - alpha/2),
                  N = 10000,
                  ps = seq(0.01, 0.99, 0.01)){

power1 <- c()
power2 <- c()
power3 <- c()

for (p in ps) {
    S <- rbinom(N, prob=p, size=n)

    tests <- (S + 1/2 * z^2) / (n + z^2)  -  z / (n + z^2) * sqrt(S * (n - S) / n + z^2 / 4) < p0 & p0 < (S + 1/2 * z^2) / (n + z^2)  +  z / (n + z^2) * sqrt(S * (n - S) / n + z^2 / 4)
    power1 <- append(power1, 1 - sum(tests) / N)

    tests <- qbeta(alpha/2, S, n - S + 1) < p0  &  p0 < qbeta(1 - alpha/2, S + 1, n - S)
    power2 <- append(power2, 1 - sum(tests) / N)

    tests <- qbeta(alpha/2, S + 0.5, n - S + 0.5) < p0  &  p0 < qbeta(1 - alpha/2, S + 0.5, n - S + 0.5)
    power3 <- append(power3, 1 - sum(tests) / N)
}

ggplot() + 
  geom_line(aes(ps, power1, col="Wilson"), linewidth=0.5, alpha = 0.4) + 
  geom_line(aes(ps, power2, col="Clopper-Pearson"), linetype="dashed", linewidth=0.7) +
  geom_line(aes(ps, power3, col="Jeffreys"), linetype="dotted", linewidth=0.8) +
  labs(x = 'p', y = 'Moc testu') + 
  scale_color_manual('', 
      breaks = c('Wilson',
                 'Clopper-Pearson',
                 'Jeffreys'
                 ),
      values = c('purple', 'lightblue', 'brown'))
}
```

```{r n7, fig.cap="\\label{fig:n7} Wykres zależności mocy testów od parametru $p$ dla $n = 7$", fig.width = 5, fig.height = 3, fig.align="center"}
n <- 7
plots(n)
```

```{r n35, fig.cap="\\label{fig:n35} Wykres zależności mocy testów od parametru $p$ dla $n = 35$", fig.width = 5, fig.height = 3, fig.align="center"}
n <- 35
plots(n)
```

```{r n250, fig.cap="\\label{fig:n250} Wykres zależności mocy testów od parametru $p$ dla $n = 250$", fig.width = 5, fig.height = 3, fig.align="center"}
n <- 250
plots(n)
```

|     Jak się okazuje, wartości mocy są identyczne dla wszystkich trzech testów. Czy oznacza to, że nie istnieje test najmocniejszy? Nie do końca. Jest to prawda dla rozważanych przez nas prób o długościach $n \in \{7, 35, 250\}$, jednak dla innych rozmiarów może być inaczej (co pokażemy w dalszej części). Zwróćmy uwagę na fakt, że to są testy oparte o przedziały ufności. Jeśli $H_0$ jest prawdziwa, to interesuje nas, z jaką częstotliwością wartość $p_0$ będzie zawarta w przedziale ufności w trakcie próby Bernoulliego. Innymi słowy, nie interesuje nas jak wąski jest przedział ufności, tylko jak często wpada do niego wartość $p_0$. Spójrzmy na wykresy przedziałów dla rozważanych długości prób ($n \in \{7, 35, 250\}$)

```{r}
intervals <- function(n, S = 0:n, alpha = 0.05){
  intervals1 <- BinomCI(S, n, method="wilson")
intervals2 <- BinomCI(S, n, method="clopper-pearson")
intervals3 <- BinomCI(S, n, method="jeffreys")

ggplot() +
  geom_point(aes(S, intervals1[,2], col="Wilson"), size = 0.3) + geom_point(aes(S, intervals1[,3], col="Wilson"), size = 0.3) +
  geom_point(aes(S, intervals2[,2], col="Clopper-Pearson"), size = 0.3) + geom_point(aes(S, intervals2[,3], col="Clopper-Pearson"), size = 0.3) +
  geom_point(aes(S, intervals3[,2], col="Jeffreys"), size = 0.3) + geom_point(aes(S, intervals3[,3], col="Jeffreys"), size = 0.3) +
  geom_line(aes(S, 0.5)) +
  labs(x = 'n', y = 'Confidence intervals') + 
  scale_color_manual('', 
      breaks = c('Wilson',
                 'Clopper-Pearson',
                 'Jeffreys'
                 ),
      values = c('purple', 'lightblue', 'brown'))
}
```

```{r conf7, fig.cap="\\label{fig:conf7} Przedziały ufności parametru $p_0$ dla $n = 7$", fig.width = 5, fig.height = 3, fig.align="center"}
intervals(7)
```

```{r conf35, fig.cap="\\label{fig:conf35} Przedziały ufności parametru $p_0$ dla $n = 35$", fig.width = 5, fig.height = 3, fig.align="center"}
intervals(35)
```

```{r conf250, fig.cap="\\label{fig:conf250} Przedziały ufności parametru $p_0$ dla $n = 250$", fig.width = 5, fig.height = 3, fig.align="center"}
intervals(250, 108:142)
```

Na powyższych wykresach wyraźnie widać, że przedział Cloppera-Pearsona jest szerszy od dwóch pozostałych. Dla zbadanych przez nas długości prób, częstotliwość wpadania $p_0$ w przedziały jest identyczna (stąd moce tych testów mają identyczne wartości dla różnych wartości $p$). Co się jednak stanie dla próby Bernoulliego o długości $n = 5$?

```{r conf5, fig.cap="\\label{fig:conf5} Przedziały ufności parametru $p_0$ dla $n = 5$. Wyraźnie widać, że wartość $p_0$ wpada najczęściej w przedział Cloppera-Pearsona", fig.width = 5, fig.height = 3, fig.align="center"}
intervals(5)
```
Możemy zauważyć, że parametr $p_0$ za każdym razem jest zawarty w przedziale Cloppera-Pearsona, podczas gdy dla pozostałych testów nie dzieje się tak dla granicznych wartości $n$. W tym przypadku to właśnie test Cloppera-Pearsona jest najmocniejszy spośród wszystkich, czego nie można było powiedzieć wcześniej.
\section{Podsumowanie}
